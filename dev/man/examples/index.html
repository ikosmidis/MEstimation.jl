<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Examples · MEstimation</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">MEstimation</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li class="is-active"><a class="tocitem" href>Examples</a><ul class="internal"><li><a class="tocitem" href="#Contents"><span>Contents</span></a></li><li><a class="tocitem" href="#Ratio-of-two-means"><span>Ratio of two means</span></a></li><li><a class="tocitem" href="#Logistic-regression"><span>Logistic regression</span></a></li><li><a class="tocitem" href="#Regularization"><span>Regularization</span></a></li></ul></li><li><span class="tocitem">Documentation</span><ul><li><a class="tocitem" href="../../lib/public/">Public</a></li><li><a class="tocitem" href="../../lib/internal/">Internal</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Examples</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Examples</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/ikosmidis/MEstimation.jl/blob/master/docs/src/man/examples.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Examples"><a class="docs-heading-anchor" href="#Examples">Examples</a><a id="Examples-1"></a><a class="docs-heading-anchor-permalink" href="#Examples" title="Permalink"></a></h1><h2 id="Contents"><a class="docs-heading-anchor" href="#Contents">Contents</a><a id="Contents-1"></a><a class="docs-heading-anchor-permalink" href="#Contents" title="Permalink"></a></h2><ul><li><a href="#Examples">Examples</a></li><li class="no-marker"><ul><li><a href="#Contents">Contents</a></li><li><a href="#Ratio-of-two-means">Ratio of two means</a></li><li><a href="#Logistic-regression">Logistic regression</a></li><li class="no-marker"><ul><li><a href="#Using-[objective_function_template](@ref)">Using <code>objective_function_template</code></a></li><li><a href="#Using-[estimating_function_template](@ref)">Using <code>estimating_function_template</code></a></li><li><a href="#Bias-reduction-methods">Bias-reduction methods</a></li></ul></li><li><a href="#Regularization">Regularization</a></li><li class="no-marker"><ul><li><a href="#Ridge-logistic-regression">Ridge logistic regression</a></li><li><a href="#Jeffreys-prior-penalty-for-bias-reduction">Jeffreys-prior penalty for bias reduction</a></li></ul></li></ul></li></ul><h2 id="Ratio-of-two-means"><a class="docs-heading-anchor" href="#Ratio-of-two-means">Ratio of two means</a><a id="Ratio-of-two-means-1"></a><a class="docs-heading-anchor-permalink" href="#Ratio-of-two-means" title="Permalink"></a></h2><p>Consider a setting where independent pairs of random variables <span>$(X_1, Y_1), \ldots, (X_n, Y_n)$</span> are observed, and suppose that interest is in the ratio of the mean of <span>$Y_i$</span> to  the mean of <span>$X_i$</span>, that is <span>$\theta = \mu_Y / \mu_X$</span>, with   <span>$\mu_X = E(X_i)$</span> and <span>$\mu_Y = E(Y_i) \ne 0$</span> <span>$(i = 1, \ldots, n)$</span>.</p><p>Assuming that sampling is from an infinite population, one way of estimating <span>$\theta$</span> without any further assumptions about the joint distribution of <span>$(X_i, Y_i)$</span> is to set the unbiased estimating equation <span>$\sum_{i = 1}^n (Y_i - \theta X_i) = 0$</span>. The resulting <span>$M$</span>-estimator is then  <span>$\hat\theta = s_Y/s_X$</span> where <span>$s_X = \sum_{i = 1}^n X_i$</span> and <span>$s_Y = \sum_{i = 1}^n Y_i$</span>. </p><p>The estimator <span>$\hat\theta$</span> is generally biased, as can be shown, for example, by an application of the Jensen inequality assuming that <span>$X_i$</span>is independent of <span>$Y_i$</span>, and its bias can be reduced using the empirically adjusted estimating functions approach in <a href="http://arxiv.org/abs/2001.03786">Kosmidis &amp; Lunardon (2020)</a>. </p><p>This example illustrates how <strong>MEstimation</strong> can be used to calculate the <span>$M$</span>-estimator and its reduced-bias version.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; using MEstimation, Random</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><p>Define a data type for ratio estimation problems</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; struct ratio_data
           y::Vector
           x::Vector
       end;</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><p>Write a function to compute the number of observations for objects of type <code>ratio_data</code>.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; function ratio_nobs(data::ratio_data)
           nx = length(data.x)
           ny = length(data.y)
           if (nx != ny)
               error(&quot;length of x is not equal to the length of y&quot;)
           end
           nx
       end;</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><p>Generate some data to test things out</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; Random.seed!(123);</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; my_data = ratio_data(randn(10), rand(10));</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ratio_nobs(my_data)</code><code class="nohighlight hljs ansi" style="display:block;">10</code></pre><p>The estimating function for the ratio <span>$\theta$</span> is </p><p><span>$\sum_{i = 1}^n (Y_i - \theta X_i)$</span></p><p>So, the contribution to the estimating function can be implemented as</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; function ratio_ef(theta::Vector,
                         data::ratio_data,
                         i::Int64)
           data.y[i] .- theta * data.x[i]
       end;</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><p>The <code>estimating_function_template</code> for the ratio estimation problem can now be set up using <code>ratio_nobs</code> and <code>ratio_ef</code>.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt;     ratio_template = estimating_function_template(ratio_nobs, ratio_ef);</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><p>We are now ready use <code>ratio_template</code> and <code>my_data</code> to compute the <span>$M$</span>-estimator of <span>$\theta$</span> by solving the estimating equation <span>$\sum_{i = 1}^n (Y_i - \theta X_i) = 0$</span>. The starting value for the nonlinear solver is set to <code>0.1</code>.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; result_m = fit(ratio_template, my_data, [0.1])</code><code class="nohighlight hljs ansi" style="display:block;">M-estimation with estimating function contributions `ratio_ef`

─────────────────────────────────────────────────────────────────────────
           Estimate  Std. Error   z value  Pr(&gt;|z|)  Lower 95%  Upper 95%
─────────────────────────────────────────────────────────────────────────
theta[1]  -0.616287    0.455728  -1.35231    0.1763    -1.5095   0.276923
─────────────────────────────────────────────────────────────────────────
Estimating functions:	[-1.4759304889366831e-12]
Converged: true</code></pre><p><code>fit</code> uses methods from the <a href="https://github.com/JuliaNLSolvers/NLsolve.jl"><strong>NLsolve</strong></a> package for solving the estimating equations. Arguments can be passed directly to <code>NLsolve.nlsolve</code> through <a href="https://docs.julialang.org/en/v1/manual/functions/#Keyword-Arguments-1">keyword arguments</a> to the <code>fit</code> method. For example,</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; result_m = fit(ratio_template, my_data, [0.1], show_trace = true)</code><code class="nohighlight hljs ansi" style="display:block;">Iter     f(x) inf-norm    Step 2-norm 
------   --------------   --------------
     0     4.044470e+00              NaN
     1     3.479826e+00     1.000000e-01
     2     2.350539e+00     2.000000e-01
     3     9.196457e-02     4.000000e-01
     4     1.475930e-12     1.628719e-02
M-estimation with estimating function contributions `ratio_ef`

─────────────────────────────────────────────────────────────────────────
           Estimate  Std. Error   z value  Pr(&gt;|z|)  Lower 95%  Upper 95%
─────────────────────────────────────────────────────────────────────────
theta[1]  -0.616287    0.455728  -1.35231    0.1763    -1.5095   0.276923
─────────────────────────────────────────────────────────────────────────
Estimating functions:	[-1.4759304889366831e-12]
Converged: true</code></pre><p>Bias reduction in general <span>$M$</span>-estimation can be achieved by solving the adjusted estimating equation <span>$\sum_{i = 1}^n (Y_i - \theta X_i) + A(\theta, Y, X) = 0$</span>, where <span>$A(\theta)$</span> are empirical bias-reducing adjustments depending on the first and second derivatives of the estimating function contributions. <strong>MEstimation</strong> can use <code>ratio_template</code> and automatic differentiation (see, <a href="https://github.com/JuliaDiff/ForwardDiff.jl">ForwardDiff</a>) to construct <span>$A(\theta, Y, X)$</span> and, then, solve the bias-reducing adjusted estimating equations. All this is simply done by</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; result_br = fit(ratio_template, my_data, [0.1], estimation_method = &quot;RBM&quot;)</code><code class="nohighlight hljs ansi" style="display:block;">RBM-estimation with estimating function contributions `ratio_ef`
Bias reduction method: implicit_trace

─────────────────────────────────────────────────────────────────────────
           Estimate  Std. Error   z value  Pr(&gt;|z|)  Lower 95%  Upper 95%
─────────────────────────────────────────────────────────────────────────
theta[1]  -0.591874    0.454331  -1.30274    0.1927   -1.48235   0.298599
─────────────────────────────────────────────────────────────────────────
Adjusted estimating functions:	[-2.853828284798965e-13]
Converged: true</code></pre><p>where <code>RBM</code> stands for reduced-bias <code>M</code>-estimation.</p><p><a href="http://arxiv.org/abs/2001.03786">Kosmidis &amp; Lunardon (2020)</a> show that the reduced-bias estimator of <span>$\theta$</span> is <span>$\tilde\theta = (s_Y + s_{XY}/s_{X})/(s_X + s_{XX}/s_{X})$</span>. The code chunks below tests that this is indeed the result <strong>MEstimation</strong> returns.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; sx = sum(my_data.x);</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; sxx = sum(my_data.x .* my_data.x);</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; sy = sum(my_data.y);</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; sxy = sum(my_data.x .* my_data.y);</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; isapprox(sy/sx, result_m.theta[1])</code><code class="nohighlight hljs ansi" style="display:block;">true</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; isapprox((sy + sxy/sx)/(sx + sxx/sx), result_br.theta[1])</code><code class="nohighlight hljs ansi" style="display:block;">true</code></pre><h2 id="Logistic-regression"><a class="docs-heading-anchor" href="#Logistic-regression">Logistic regression</a><a id="Logistic-regression-1"></a><a class="docs-heading-anchor-permalink" href="#Logistic-regression" title="Permalink"></a></h2><h3 id="Using-[objective_function_template](@ref)"><a class="docs-heading-anchor" href="#Using-[objective_function_template](@ref)">Using <a href="../../lib/public/#MEstimation.objective_function_template"><code>objective_function_template</code></a></a><a id="Using-[objective_function_template](@ref)-1"></a><a class="docs-heading-anchor-permalink" href="#Using-[objective_function_template](@ref)" title="Permalink"></a></h3><p>Here, we use <strong>MEstimation</strong>&#39;s <a href="../../lib/public/#MEstimation.objective_function_template"><code>objective_function_template</code></a> to estimate a logistic regression model using maximum likelihood and maximum penalized likelihood, with the empirical bias-reducing penalty in <a href="http://arxiv.org/abs/2001.03786">Kosmidis &amp; Lunardon (2020)</a>.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; using MEstimation</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; using Random</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; using Distributions</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; using Optim</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><p>A data type for logistic regression models (consisting of a response vector <code>y</code>, a model matrix <code>x</code>, and a vector of weights <code>m</code>) is</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; struct logistic_data
           y::Vector
           x::Array{Float64}
           m::Vector
       end</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><p>A function to compute the number of observations from <code>logistic_data</code> objects is</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; function logistic_nobs(data::logistic_data)
           nx = size(data.x)[1]
           ny = length(data.y)
           nm = length(data.m)
           if (nx != ny)
               error(&quot;number of rows in of x is not equal to the length of y&quot;)
           elseif (nx != nm)
               error(&quot;number of rows in of x is not equal to the length of m&quot;)
           elseif (ny != nm)
               error(&quot;length of y is not equal to the length of m&quot;)
           end
           nx
       end</code><code class="nohighlight hljs ansi" style="display:block;">logistic_nobs (generic function with 1 method)</code></pre><p>The logistic regression log-likelihood contribution at a parameter <code>theta</code> for the <span>$i$</span>th observations of data <code>data</code> is</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; function logistic_loglik(theta::Vector,
                                data::logistic_data,
                                i::Int64)
           eta = sum(data.x[i, :] .* theta)
           mu = exp.(eta)./(1 .+ exp.(eta))
           data.y[i] .* log.(mu) + (data.m[i] - data.y[i]) .* log.(1 .- mu)
       end</code><code class="nohighlight hljs ansi" style="display:block;">logistic_loglik (generic function with 1 method)</code></pre><p>Let&#39;s simulate some logistic regression data with <span>$10$</span> covariates</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; Random.seed!(123);</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; n = 100;</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; m = 1;</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; p = 10</code><code class="nohighlight hljs ansi" style="display:block;">10</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; x = Array{Float64}(undef, n, p);</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; x[:, 1] .= 1.0;</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; for j in 2:p
               x[:, j] .= rand(n);
       end</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; true_betas = randn(p) * sqrt(p);</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; y = rand.(Binomial.(m, cdf.(Logistic(), x * true_betas)));</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; my_data = logistic_data(y, x, fill(m, n));</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><p>and set up an <code>objective_function_template</code> for logistic regression</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; logistic_template = objective_function_template(logistic_nobs, logistic_loglik)</code><code class="nohighlight hljs ansi" style="display:block;">objective_function_template(Main.logistic_nobs, Main.logistic_loglik)</code></pre><p>The maximum likelihood estimates starting at <code>true_betas</code> are</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; o1_ml = fit(logistic_template, my_data, true_betas, optim_method = NelderMead())</code><code class="nohighlight hljs ansi" style="display:block;">M-estimation with objective contributions `logistic_loglik`

──────────────────────────────────────────────────────────────────────────────
              Estimate  Std. Error     z value  Pr(&gt;|z|)  Lower 95%  Upper 95%
──────────────────────────────────────────────────────────────────────────────
theta[1]     0.0642275     5.40987   0.0118723    0.9905  -10.5389    10.6674
theta[2]    -1.07189       2.61211  -0.410355     0.6815   -6.19154    4.04775
theta[3]   -10.8887        2.51036  -4.33751      &lt;1e-04  -15.8089    -5.96849
theta[4]   -17.4507        5.02228  -3.47466      0.0005  -27.2942    -7.60723
theta[5]   -10.1597        3.76219  -2.70048      0.0069  -17.5335    -2.78595
theta[6]    -6.67685       1.98707  -3.36015      0.0008  -10.5714    -2.78226
theta[7]   -23.5455        3.61585  -6.51176      &lt;1e-10  -30.6325   -16.4586
theta[8]   -30.1538        6.29408  -4.79082      &lt;1e-05  -42.49     -17.8176
theta[9]    -0.11266       1.67223  -0.0673712    0.9463   -3.39017    3.16485
theta[10]   -0.219789      2.30336  -0.0954212    0.9240   -4.73429    4.29471
──────────────────────────────────────────────────────────────────────────────
Objective:			-0.0
Takeuchi information criterion:	0.0
Akaike information criterion:	20.0
Converged: true</code></pre><p><code>fit</code> uses methods from the <a href="https://github.com/JuliaNLSolvers/Optim.jl"><strong>Optim</strong></a> package internally. Here, we used the <code>Optim.NelderMead</code> method. Alternative optimization methods and options can be supplied directly through the <a href="https://docs.julialang.org/en/v1/manual/functions/#Keyword-Arguments-1">keyword arguments</a> <code>optim_method</code> and <code>optim_options</code>, respectively. For example,</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; o2_ml = fit(logistic_template, my_data, true_betas, optim_method = LBFGS(), optim_options = Optim.Options(g_abstol = 1e-05))</code><code class="nohighlight hljs ansi" style="display:block;">M-estimation with objective contributions `logistic_loglik`

──────────────────────────────────────────────────────────────────────────
            Estimate  Std. Error   z value  Pr(&gt;|z|)  Lower 95%  Upper 95%
──────────────────────────────────────────────────────────────────────────
theta[1]   -18.2474      3.85573  -4.73253    &lt;1e-05   -25.8045  -10.6903
theta[2]    -9.26473     2.25983  -4.09974    &lt;1e-04   -13.6939   -4.83554
theta[3]   -11.3257      2.12791  -5.32243    &lt;1e-06   -15.4963   -7.15503
theta[4]    -8.88823     2.69686  -3.29577    0.0010   -14.174    -3.60249
theta[5]    -9.00095     2.85915  -3.14812    0.0016   -14.6048   -3.39712
theta[6]    -8.9545      1.49549  -5.98768    &lt;1e-08   -11.8856   -6.0234
theta[7]   -11.8706      2.59587  -4.57288    &lt;1e-05   -16.9584   -6.7828
theta[8]   -10.5582      1.87893  -5.61924    &lt;1e-07   -14.2408   -6.87552
theta[9]   -11.9214      2.11974  -5.62401    &lt;1e-07   -16.076    -7.76681
theta[10]   -7.91036     2.47638  -3.19433    0.0014   -12.764    -3.05675
──────────────────────────────────────────────────────────────────────────
Objective:			0.0
Takeuchi information criterion:	0.0
Akaike information criterion:	20.0
Converged: true</code></pre><p>The reduced-bias estimates starting at the maximum likelihood ones are</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; o1_br = fit(logistic_template, my_data, coef(o1_ml), estimation_method = &quot;RBM&quot;)</code><code class="nohighlight hljs ansi" style="display:block;">RBM-estimation with objective contributions `logistic_loglik`
Bias reduction method: implicit_trace

──────────────────────────────────────────────────────────────────────────────
              Estimate  Std. Error     z value  Pr(&gt;|z|)  Lower 95%  Upper 95%
──────────────────────────────────────────────────────────────────────────────
theta[1]     0.0642275     5.40987   0.0118723    0.9905  -10.5389    10.6674
theta[2]    -1.07189       2.61211  -0.410355     0.6815   -6.19154    4.04775
theta[3]   -10.8887        2.51036  -4.33751      &lt;1e-04  -15.8089    -5.96849
theta[4]   -17.4507        5.02228  -3.47466      0.0005  -27.2942    -7.60723
theta[5]   -10.1597        3.76219  -2.70048      0.0069  -17.5335    -2.78595
theta[6]    -6.67685       1.98707  -3.36015      0.0008  -10.5714    -2.78226
theta[7]   -23.5455        3.61585  -6.51176      &lt;1e-10  -30.6325   -16.4586
theta[8]   -30.1538        6.29408  -4.79082      &lt;1e-05  -42.49     -17.8176
theta[9]    -0.11266       1.67223  -0.0673712    0.9463   -3.39017    3.16485
theta[10]   -0.219789      2.30336  -0.0954212    0.9240   -4.73429    4.29471
──────────────────────────────────────────────────────────────────────────────
Penalized objetive:		-0.0
Takeuchi information criterion:	0.0
Akaike information criterion:	20.0
Converged: true</code></pre><h3 id="Using-[estimating_function_template](@ref)"><a class="docs-heading-anchor" href="#Using-[estimating_function_template](@ref)">Using <a href="../../lib/public/#MEstimation.estimating_function_template"><code>estimating_function_template</code></a></a><a id="Using-[estimating_function_template](@ref)-1"></a><a class="docs-heading-anchor-permalink" href="#Using-[estimating_function_template](@ref)" title="Permalink"></a></h3><p>The same results as above can be returned using an <a href="../../lib/public/#MEstimation.estimating_function_template"><code>estimating_function_template</code></a> for logistic regression. </p><p>The contribution to the derivatives of the log-likelihood for logistic regression is</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; function logistic_ef(theta::Vector,
                            data::logistic_data,
                            i::Int64)
           eta = sum(data.x[i, :] .* theta)
           mu = exp.(eta)./(1 .+ exp.(eta))
           data.x[i, :] * (data.y[i] - data.m[i] * mu)
       end</code><code class="nohighlight hljs ansi" style="display:block;">logistic_ef (generic function with 1 method)</code></pre><p>Then, solving the bias-reducing adjusted estimating equations</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; logistic_template_ef = estimating_function_template(logistic_nobs, logistic_ef);</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; e1_br = fit(logistic_template_ef, my_data, true_betas, estimation_method = &quot;RBM&quot;)</code><code class="nohighlight hljs ansi" style="display:block;">RBM-estimation with estimating function contributions `logistic_ef`
Bias reduction method: implicit_trace

──────────────────────────────────────────────────────────────────────────────
             Estimate  Std. Error    z value  Pr(&gt;|z|)   Lower 95%   Upper 95%
──────────────────────────────────────────────────────────────────────────────
theta[1]   -17.5124      2.088     -8.38719     &lt;1e-16  -21.6048    -13.42
theta[2]     1.2354      1.13902    1.08461     0.2781   -0.997051    3.46784
theta[3]    -3.8147      1.11491   -3.42154     0.0006   -5.99988    -1.62953
theta[4]     3.79134     1.05689    3.58725     0.0003    1.71987     5.8628
theta[5]    -3.69434     1.40476   -2.62988     0.0085   -6.44761    -0.941064
theta[6]    -2.55227     1.50629   -1.69441     0.0902   -5.50453     0.400002
theta[7]    -6.82179     1.88731   -3.61457     0.0003  -10.5208     -3.12274
theta[8]    -6.41405     1.33859   -4.79163     &lt;1e-05   -9.03764    -3.79045
theta[9]    -0.422571    0.928138  -0.455288    0.6489   -2.24169     1.39655
theta[10]   -0.235556    1.2173    -0.193507    0.8466   -2.62141     2.1503
──────────────────────────────────────────────────────────────────────────────
Adjusted estimating functions:	[-7.760109872099276e-9, -4.655475157399112e-9, -3.1595236182836216e-9, -5.6671279720315534e-9, -2.217336561400475e-9, -2.7053000608775576e-9, -2.2060763258217927e-9, -1.7088852877251828e-9, -5.208711227184449e-9, -3.3428348041037216e-9]
Converged: true</code></pre><p>returns the reduced-bias estimates from maximum penalized likelihood:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; isapprox(coef(o1_br), coef(e1_br))</code><code class="nohighlight hljs ansi" style="display:block;">false</code></pre><h3 id="Bias-reduction-methods"><a class="docs-heading-anchor" href="#Bias-reduction-methods">Bias-reduction methods</a><a id="Bias-reduction-methods-1"></a><a class="docs-heading-anchor-permalink" href="#Bias-reduction-methods" title="Permalink"></a></h3><p><strong>MEstimation</strong> currently implements 2 alternative bias reduction methods, called <code>implicit_trace</code> and <code>explicit_trace</code>. <code>implicit_trace</code> will adjust the estimating functions or penalize the objectives, as we have seen earlier. <code>explicit_trace</code>, on the other hand, will form an estimate of the bias of the <span>$M$</span>-estimator and subtract that from the <span>$M$</span>-estimates. The default method is <code>implicit_trace</code>.</p><p>For example, for logistic regression via estimating functions </p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; e2_br = fit(logistic_template_ef, my_data, true_betas, estimation_method = &quot;RBM&quot;, br_method = &quot;explicit_trace&quot;)</code><code class="nohighlight hljs ansi" style="display:block;">RBM-estimation with estimating function contributions `logistic_ef`
Bias reduction method: explicit_trace

────────────────────────────────────────────────────────────────────────────────
             Estimate  Std. Error     z value  Pr(&gt;|z|)   Lower 95%    Upper 95%
────────────────────────────────────────────────────────────────────────────────
theta[1]   -18.7594      1.47295   -12.7359      &lt;1e-36  -21.6464    -15.8725
theta[2]     0.980103    0.909501    1.07763     0.2812   -0.802485    2.76269
theta[3]    -3.11008     0.969043   -3.20944     0.0013   -5.00937    -1.21079
theta[4]     3.22469     0.914142    3.52756     0.0004    1.43301     5.01638
theta[5]    -2.92686     1.25522    -2.33175     0.0197   -5.38705    -0.466676
theta[6]    -2.12556     1.10169    -1.92937     0.0537   -4.28484     0.0337069
theta[7]    -5.22537     1.23829    -4.21982     &lt;1e-04   -7.65238    -2.79836
theta[8]    -5.09395     0.937559   -5.43321     &lt;1e-07   -6.93154    -3.25637
theta[9]    -0.38592     0.75559    -0.510754    0.6095   -1.86685     1.09501
theta[10]   -0.219085    0.976058   -0.224459    0.8224   -2.13212     1.69395
────────────────────────────────────────────────────────────────────────────────
Adjusted estimating functions:	[-5.47446090960202e-9, -3.2215350169080164e-9, -2.340309274344094e-9, -3.879654537639649e-9, -1.6620069064723074e-9, -1.9809377243771136e-9, -1.570939043088554e-9, -1.3081671328038098e-9, -3.486520882802875e-9, -2.3720147832941833e-9]
Converged: true</code></pre><p>which gives slightly different estimates that what are in the <code>implict_trace</code> fit in <code>e1_br</code>. </p><p>The same can be done using objective functions, but numerical differentiation (using the <a href="https://github.com/JuliaDiff/FiniteDiff.jl">FiniteDiff</a> package) is used to approximate the gradient of the bias-reducing penalty (i.e. <span>$A(\theta)$</span>).</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; o2_br = fit(logistic_template, my_data, true_betas, estimation_method = &quot;RBM&quot;, br_method = &quot;explicit_trace&quot;)</code><code class="nohighlight hljs ansi" style="display:block;">RBM-estimation with objective contributions `logistic_loglik`
Bias reduction method: explicit_trace

──────────────────────────────────────────────────────────────────────────
            Estimate  Std. Error   z value  Pr(&gt;|z|)  Lower 95%  Upper 95%
──────────────────────────────────────────────────────────────────────────
theta[1]   -19.6885      3.81435  -5.16169    &lt;1e-06   -27.1645  -12.2125
theta[2]    -8.82874     2.04152  -4.3246     &lt;1e-04   -12.83     -4.82744
theta[3]   -10.4424      2.01936  -5.17116    &lt;1e-06   -14.4003   -6.48456
theta[4]    -8.94337     2.5234   -3.54418    0.0004   -13.8891   -3.9976
theta[5]    -9.08163     2.86062  -3.1747     0.0015   -14.6884   -3.47491
theta[6]    -9.20376     1.50091  -6.13212    &lt;1e-09   -12.1455   -6.26203
theta[7]   -11.4         2.5927   -4.39696    &lt;1e-04   -16.4816   -6.3184
theta[8]   -10.5003      1.80892  -5.80476    &lt;1e-08   -14.0458   -6.95492
theta[9]   -10.6199      1.75331  -6.05704    &lt;1e-08   -14.0563   -7.18346
theta[10]   -6.96344     2.27247  -3.06425    0.0022   -11.4174   -2.50947
──────────────────────────────────────────────────────────────────────────
Penalized objetive:		-0.0
Takeuchi information criterion:	0.0
Akaike information criterion:	20.0
Converged: true</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; isapprox(coef(e2_br), coef(o2_br))</code><code class="nohighlight hljs ansi" style="display:block;">false</code></pre><h2 id="Regularization"><a class="docs-heading-anchor" href="#Regularization">Regularization</a><a id="Regularization-1"></a><a class="docs-heading-anchor-permalink" href="#Regularization" title="Permalink"></a></h2><p><strong>MEstimation</strong> allows to pass arbitrary regularizers to either the objective or the estimating functions. Below we illustrate that functionality for carrying out ridge logistic regression, and maximum penalized likelihood, with a Jeffreys-prior penalty.</p><h3 id="Ridge-logistic-regression"><a class="docs-heading-anchor" href="#Ridge-logistic-regression">Ridge logistic regression</a><a id="Ridge-logistic-regression-1"></a><a class="docs-heading-anchor-permalink" href="#Ridge-logistic-regression" title="Permalink"></a></h3><p>The <code>logistic_template</code> that we defined earlier can be used for doing L2-regularized logistic regression (aka ridge logistic regression); we only need to define a function that implements the L2 regularizer</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; l2_penalty = (theta, data, λ) -&gt; - λ * sum(theta.^2);</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><p>Then, the coefficient path can be computed as</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; lambda = collect(0:0.5:10);</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; deviance = similar(lambda);</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; coefficients = Matrix{Float64}(undef, length(lambda), length(true_betas));</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; coefficients[1, :] = coef(o1_ml);</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; for j in 2:length(lambda)
           current_fit = fit(logistic_template, my_data, coefficients[j - 1, :],
                             regularizer = (theta, data) -&gt; l2_penalty(theta, data, lambda[j]))
           deviance[j] = 2 * current_fit.results.minimum
           coefficients[j, :] = coef(current_fit)
       end</code><code class="nohighlight hljs ansi" style="display:block;"></code></pre><p>The coefficients versus <span>$\lambda$</span>, and the deviance values are then</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; using Plots</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: ArgumentError: Package Plots not found in current path.
- Run `import Pkg; Pkg.add(&quot;Plots&quot;)` to install the Plots package.</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; plot(lambda, coefficients);</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: UndefVarError: plot not defined</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; savefig(&quot;coef_path1.svg&quot;);</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: UndefVarError: savefig not defined</code></pre><p><img src="coef_path1.svg" alt/></p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; plot(deviance, coefficients);</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: UndefVarError: plot not defined</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; savefig(&quot;coef_path2.svg&quot;);</code><code class="nohighlight hljs ansi" style="display:block;">ERROR: UndefVarError: savefig not defined</code></pre><p><img src="coef_path2.svg" alt/></p><p>Another way to get the above is to define a new data type that has a filed for <span>$\lambda$</span> and then pass</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; l2_penalty = (theta, data) -&gt; - data.λ * sum(theta.^2)</code><code class="nohighlight hljs ansi" style="display:block;">#5 (generic function with 1 method)</code></pre><p>to the <code>regularizer</code> argument when calling <code>fit</code>. Such a new data type, though, would require to redefine <code>logistic_loglik</code>, <code>logistic_nobs</code> and <code>logistic_template</code>.</p><h3 id="Jeffreys-prior-penalty-for-bias-reduction"><a class="docs-heading-anchor" href="#Jeffreys-prior-penalty-for-bias-reduction">Jeffreys-prior penalty for bias reduction</a><a id="Jeffreys-prior-penalty-for-bias-reduction-1"></a><a class="docs-heading-anchor-permalink" href="#Jeffreys-prior-penalty-for-bias-reduction" title="Permalink"></a></h3><p><a href="https://www.jstor.org/stable/2336755">Firth (1993)</a> showed that an alternative bias-reducing penalty for the logistic regression likelihood is the Jeffreys prior,. which can readily implemented and passed to <code>fit</code> through the <code>regularizer</code> interface that <strong>MEstimation</strong> provides. The logarithm of the Jeffreys prior for logistic regression is </p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; using LinearAlgebra</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; function log_jeffreys_prior(theta, data)
           x = data.x
           probs = cdf.(Logistic(), x * theta)
           log(det((x .* (data.m .* probs .* (1 .- probs)))&#39; * x)) / 2
       end</code><code class="nohighlight hljs ansi" style="display:block;">log_jeffreys_prior (generic function with 1 method)</code></pre><p>Then, the reduced-bias estimates of <a href="https://www.jstor.org/stable/2336755">Firth (1993)</a> are</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; o_jeffreys = fit(logistic_template, my_data, true_betas, regularizer = log_jeffreys_prior)</code><code class="nohighlight hljs ansi" style="display:block;">M-estimation with objective contributions `logistic_loglik` and user-supplied regularizer

───────────────────────────────────────────────────────────────────────────
            Estimate  Std. Error    z value  Pr(&gt;|z|)  Lower 95%  Upper 95%
───────────────────────────────────────────────────────────────────────────
theta[1]   -2.72139     0.591575  -4.60025     &lt;1e-05  -3.88086   -1.56193
theta[2]   -0.709242    0.413271  -1.71616     0.0861  -1.51924    0.100755
theta[3]   -0.15944     0.402685  -0.395942    0.6921  -0.948687   0.629808
theta[4]    0.29656     0.408597   0.725801    0.4680  -0.504275   1.09739
theta[5]    0.329316    0.410825   0.801597    0.4228  -0.475886   1.13452
theta[6]   -0.222871    0.359498  -0.61995     0.5353  -0.927473   0.481732
theta[7]    0.679226    0.408675   1.66202     0.0965  -0.121763   1.48021
theta[8]   -0.292296    0.392618  -0.744479    0.4566  -1.06181    0.477221
theta[9]   -0.159811    0.378344  -0.422395    0.6727  -0.90135    0.581729
theta[10]  -0.392374    0.39982   -0.981378    0.3264  -1.17601    0.391258
───────────────────────────────────────────────────────────────────────────
Objective:			-4.6144
Takeuchi information criterion:	10.2889
Akaike information criterion:	29.2289
Converged: true</code></pre><p>Note here, that the <code>regularizer</code> is only used to get estimates. Then all model quantities are computed at those estimates, but based only on <code>logistic_loglik</code> (i.e. without adding the regularizer to it). <a href="http://arxiv.org/abs/1812.01938">Kosmidis &amp; Firth (2020)</a> provide a more specific procedure for computing the reduced-bias estimates from the penalization of the logistic regression likelihood by Jeffreys prior, which uses repeated maximum likelihood fits on adjusted binomial data. <a href="http://arxiv.org/abs/1812.01938">Kosmidis &amp; Firth (2020)</a> also show that, for logistic regression, the reduced-bias estimates from are always finite and shrink to zero relative to the maximum likelihood estimator.</p><p>Regularization is also available when fitting an <code>estimating_function_template</code>. For example, the gradient of the <code>log_jeffreys_prior</code> above is</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; using ForwardDiff</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; log_jeffreys_prior_grad = (theta, data) -&gt; ForwardDiff.gradient(pars -&gt; log_jeffreys_prior(pars, data), theta)</code><code class="nohighlight hljs ansi" style="display:block;">#7 (generic function with 1 method)</code></pre><p>Then the same fit as <code>o_jeffreys</code> can be obtained using estimating functions</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; e_jeffreys = fit(logistic_template_ef, my_data, true_betas, regularizer = log_jeffreys_prior_grad)</code><code class="nohighlight hljs ansi" style="display:block;">M-estimation with estimating function contributions `logistic_ef` and user-supplied regularizer

───────────────────────────────────────────────────────────────────────────────
              Estimate  Std. Error     z value  Pr(&gt;|z|)   Lower 95%  Upper 95%
───────────────────────────────────────────────────────────────────────────────
theta[1]    -2.79797       3.53135  -0.792324     0.4282   -9.71929     4.12335
theta[2]     5.85177       4.92341   1.18856      0.2346   -3.79793    15.5015
theta[3]    -5.54879       2.09062  -2.65413      0.0080   -9.64633    -1.45125
theta[4]     9.56232       5.13043   1.86384      0.0623   -0.493138   19.6178
theta[5]   -20.4785        8.32226  -2.46069      0.0139  -36.7898     -4.1672
theta[6]     6.38901       3.73089   1.71246      0.0868   -0.923392   13.7014
theta[7]   -17.6612        8.18714  -2.15719      0.0310  -33.7077     -1.61472
theta[8]   -10.7012        4.61038  -2.3211       0.0203  -19.7373     -1.66499
theta[9]     3.19947       2.44817   1.30688      0.1913   -1.59885     7.9978
theta[10]    0.0667298     2.92441   0.0228182    0.9818   -5.66501     5.79847
───────────────────────────────────────────────────────────────────────────────
Estimating functions:	[-2.5499385846674096, -1.2267080678383444, -1.5814116404465246, -1.6263514421542489, -0.46127405373866276, -1.6300048300256176, -0.3886970820472852, -0.7169691379612477, -1.4753766668065311, -1.0386761122985442]
Converged: false</code></pre><p>Note here that the value of the estimating functions shown in the output is that of the gradient of the log-likelihood, i.e.</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; logistic_loglik_grad = estimating_function(coef(e_jeffreys), my_data, logistic_template_ef)</code><code class="nohighlight hljs ansi" style="display:block;">10-element Vector{Float64}:
 -2.5499385846674096
 -1.2267080678383444
 -1.5814116404465246
 -1.6263514421542489
 -0.46127405373866276
 -1.6300048300256176
 -0.3886970820472852
 -0.7169691379612477
 -1.4753766668065311
 -1.0386761122985442</code></pre><p>instead of the regularized estimating functions, which, as expected, are very close to zero at the estimates</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; logistic_loglik_grad .+ log_jeffreys_prior_grad(coef(e_jeffreys), my_data)</code><code class="nohighlight hljs ansi" style="display:block;">10-element Vector{Float64}:
  0.008001676862369855
  0.0022977566545099304
  0.018539029846736055
 -0.08383973088877483
  0.08302982327507508
 -0.03534398545997175
  0.06969364460488003
  0.066260115156075
  0.00815357954767415
  0.014606402949612463</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../">« Home</a><a class="docs-footer-nextpage" href="../../lib/public/">Public »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.22 on <span class="colophon-date" title="Thursday 1 September 2022 16:09">Thursday 1 September 2022</span>. Using Julia version 1.8.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
