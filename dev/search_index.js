var documenterSearchIndex = {"docs":
[{"location":"lib/public/#Public-documentation","page":"Public","title":"Public documentation","text":"","category":"section"},{"location":"lib/public/#Contents","page":"Public","title":"Contents","text":"","category":"section"},{"location":"lib/public/","page":"Public","title":"Public","text":"Pages = [\"public.md\"]","category":"page"},{"location":"lib/public/#Index","page":"Public","title":"Index","text":"","category":"section"},{"location":"lib/public/","page":"Public","title":"Public","text":"Pages = [\"public.md\"]","category":"page"},{"location":"lib/public/#Public-interface","page":"Public","title":"Public interface","text":"","category":"section"},{"location":"lib/public/","page":"Public","title":"Public","text":"estimating_function_template\nget_estimating_function\nestimating_function\nobjective_function_template\nobjective_function\nfit(template::objective_function_template, data::Any, theta::Vector{Float64}; estimation_method::String = \"M\", br_method::String = \"implicit_trace\", regularizer::Function = function regularizer(theta::Vector{Float64}, data::Any) Vector{Float64}() end, lower::Vector{Float64} = Vector{Float64}(), upper::Vector{Float64} = Vector{Float64}(), optim_method = LBFGS(), optim_options = Optim.Options(), optim_arguments...)\nfit(template::estimating_function_template, data::Any, theta::Vector{Float64}; estimation_method::String = \"M\", br_method::String = \"implicit_trace\", concentrate::Vector{Int64} = Vector{Int64}(), regularizer::Function = function regularizer(theta::Vector{Float64}, data::Any) Vector{Float64}() end, nlsolve_arguments...)\ncoef\nvcov\nstderror\ncoeftable\ntic\naic\nslice","category":"page"},{"location":"lib/public/#MEstimation.estimating_function_template","page":"Public","title":"MEstimation.estimating_function_template","text":"estimating_function_template(nobs::Function, \n                             ef_contribution::Function)\n\nComposite type for defining an estimating_function_template.\n\nArguments\n\nnobs: a function of data that computes the number of observations of the particular data type,\nef_contribution: a function of the parameters theta, the data and the observation index i that returns a vector of length length(theta).\n\nResult\n\nAn estimating_function_template object with fields nobs and obj_contributions.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#MEstimation.get_estimating_function","page":"Public","title":"MEstimation.get_estimating_function","text":"get_estimating_function(data::Any,\n                        template::estimating_function_template,\n                        br::Bool = false,\n                        concentrate::Vector{Int64} = Vector{Int64}(),\n                        regularizer::Any = Vector{Int64}())\n\nConstruct the estimating functions by adding up all contributions in the data according to estimating_function_template.\n\nArguments\n\ndata: typically an object of composite type with all the data required to compute the estimating_function.\ntemplate: an estimating_function_template object.\nbr: a Bool. If false (default), the estimating functions is constructed by adding up all contributions in \n\ndata, according to estimating_function_template, before it is evaluated at theta. If true then the empirical bias-reducing adjustments in Kosmidis & Lunardon, 2020 are computed and added to the estimating functions.\n\nconcentrate: a Vector{Int64}; if specified, empirical bias-reducing adjustments are added only to the subset of estimating functions indexed by concentrate. The default is to add empirical bias-reducing adjustments to all estimating functions.\nregularizer: a function of theta and data returning a Vector of dimension equal to the number of the estimating functions, which is added to the (bias-reducing) estimating function; the default value will result in no regularization.\n\nResult\n\nAn in-place function that stores the value of the estimating functions inferred from template, in a preallocated vector passed as its first argument, ready to be used withing NLsolve.nlsolve. This is the in-place version of estimating_function with the extra regularizer argument.\n\n\n\n\n\n","category":"function"},{"location":"lib/public/#MEstimation.estimating_function","page":"Public","title":"MEstimation.estimating_function","text":"estimating_function(theta::Vector,\n                    data::Any,\n                    template::estimating_function_template,\n                    br::Bool = false,\n                    concentrate::Vector{Int64} = Vector{Int64}())\n\nEvaluate a vector of estimating functions at theta by adding up all contributions in data, according to an estimating_function_template.\n\nArguments\n\ntheta: a Vector of parameter values at which to evaluate the estimating functions\ndata: typically an object of composite type with all the data required to compute the estimating_function.\ntemplate: an estimating_function_template object.\nbr: a Bool. If false (default), the estimating functions is constructed by adding up all contributions in \n\ndata, according to estimating_function_template, before it is evaluated at theta. If true then the empirical bias-reducing adjustments in Kosmidis & Lunardon, 2020 are computed and added to the estimating functions.\n\nconcentrate: a Vector{Int64}; if specified, empirical bias-reducing adjustments are added only to the subset of estimating functions indexed by concentrate. The default is to add empirical bias-reducing adjustments to all estimating functions.\n\nResult\n\nA Vector.\n\nDetails\n\ndata can be used to pass additional constants other than the actual data to the objective.\n\n\n\n\n\n","category":"function"},{"location":"lib/public/#MEstimation.objective_function_template","page":"Public","title":"MEstimation.objective_function_template","text":"objective_function_template(nobs::Function, \n                            obj_contribution::Function)\n\nA constructor of objects of composite type for defining an objective_function_template.\n\nArguments\n\nnobs: a function of data that computes the number of observations of the particular data type,\nobj_contribution: a function of the parameters theta, the data and the observation index i that returns a Float64.\n\nResult\n\nAn objective_function_template object with fields nobs and obj_contributions.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#MEstimation.objective_function","page":"Public","title":"MEstimation.objective_function","text":"objective_function(theta::Vector, \n                   data::Any, \n                   template::objective_function_template, \n                   br::Bool = false)\n\nEvaluates the objective function at theta by adding up all contributions in  data, according to objective_function_template.\n\nArguments\n\ntheta: a Vector of parameter values at which to evaluate the objective function\ndata: typically an object of composite type with all the data required to compute the objective_function.\ntemplate: an objective_function_template object\nbr: a Bool. If false (default), the objective function is constructed by adding up all contributions in \n\ndata, according to objective_function_template, before it is evaluated at theta. If true then the bias-reducing penalty in Kosmidis & Lunardon, 2020 is computed and added to the objective function.\n\nResult\n\nA Float64.\n\nDetails\n\ndata can be used to pass additional constants other than the actual data to the objective. \n\n\n\n\n\n","category":"function"},{"location":"lib/public/#StatsAPI.fit-Tuple{objective_function_template, Any, Vector{Float64}}","page":"Public","title":"StatsAPI.fit","text":"fit(template::objective_function_template,\n    data::Any,\n    theta::Vector{Float64};\n    estimation_method::String = \"M\",\n    br_method::String = \"implicit_trace\",\n    regularizer::Function = function regularizer(theta::Vector{Float64}, data::Any) Vector{Float64}() end,\n    lower::Vector{Float64} = Vector{Float64}(),\n    upper::Vector{Float64} = Vector{Float64}(),\n    optim_method = LBFGS(),\n    optim_options = Optim.Options(),\n    optim_arguments...)\n\nFit an objective_function_template on data using M-estimation (estimation_method = \"M\"; default) or RBM-estimation (reduced-bias M estimation; Kosmidis & Lunardon, 2020; estimation_method = \"RBM\")\n\nArguments\n\ntemplate: an objective_function_template object.\ndata: typically an object of composite type with all the data required to compute the objective_function.\ntheta: a Vector{Float64} of parameter values to use as starting values in Optim.optimize.\n\nKeyword arguments\n\nestimation_method: either \"M\" (default) or \"RBM\"; see Details.\nbr_method: either \"implicittrace\" (default) or \"explicittrace\"; see Details.\nregularizer: a function of theta and data returning a Float64, which is added to the (bias-reducing penalized) objective; the default value will result in no regularization.\nlower: a Vector{Float64} of dimension equal to theta for setting box constraints for the optimization. The default will result in unconstrained optimization. See Details.\nupper: a Vector{Float64} of dimension equal to theta for setting box constraints for the optimization. The default will result in unconstrained optimization. See Details.\noptim_method: the optimization method to be used; deafult is Optim.LBFGS(). See Details.\noptim_options: the result of a call to Optim.Options to be passed to Optim.optimize. Default is Optim.Options(). See details.\noptim_arugments...: extra keyword arguments to be passed to Optim.optimize. See Details.\n\nDetails\n\nBias reduction is either through the maximization of the bias-reducing penalized objective in Kosmidis & Lunardon (2020) (br_method = \"implicit_trace\"; default) or by subtracting an estimate of the bias from the M-estimates (br_method = \"explicit_trace\"). The bias-reducing penalty is constructed internally using automatic differentiation (using the ForwardDiff package), and the bias estimate using a combination of automatic differentiation and numerical differentiation (using the FiniteDiff package).\n\nThe maximization of the objective or the penalized objective is done using the Optim package. Optimization methods and options can be supplied directly through the optim_method and optim_options, respectively. optim_options expects an object constructed through Optim.Options. Keyword arguments (e.g. autodiff = :forward) can be passed directly to Optim.optimize through extra keyword arguments. See the Optim documentation for more details on the available options.\n\nAn extra additive regularizer to either the objective or the bias-reducing penalized objective can be suplied via the keyword argument regularizer, which must be a scalar-valued function of the parameters and the data; the default value will result in no regularization.\n\nlower and upper can be used to provide box contraints. If valid lower and upper vectors are supplier, then the internal call to Optim.optimize will use Fminbox(optim_method) as a method; see the Optim documentation on box minimization for more details.\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#StatsAPI.fit-Tuple{estimating_function_template, Any, Vector{Float64}}","page":"Public","title":"StatsAPI.fit","text":"fit(template::estimating_function_template,\n    data::Any,\n    theta::Vector{Float64};\n    estimation_method::String = \"M\",\n    br_method::String = \"implicit_trace\",\n    concentrate::Vector{Int64} = Vector{Int64}(),\n    regularizer::Function = function regularizer(theta::Vector{Float64}, data::Any) Vector{Float64}() end,\n    nlsolve_arguments...)\n\nFit an estimating_function_template on data using M-estimation (estimation_method = \"M\"; default) or RBM-estimation (reduced-bias M estimation; Kosmidis & Lunardon, 2020; estimation_method = \"RBM\")\n\nArguments\n\ntemplate: an estimating_function_template object.\ndata: typically an object of composite type with all the data required to compute the objective_function.\ntheta: a Vector{Float64} of parameter values to use as starting values in Optim.optimize.\n\nKeyword arguments\n\nestimation_method: either \"M\" (default) or \"RBM\"; see Details.\nbr_method: either \"implicittrace\" (default) or \"explicittrace\"; see Details.\nconcentrate: a Vector{Int64}; if specified, empirical bias-reducing adjustments are added only to the subset of estimating functions indexed by concentrate. The default is to add empirical bias-reducing adjustments to all estimating functions.\nregularizer: a function of theta and data returning a Vector{Float64} of dimension equal to the number of the estimating functions, which is added to the (bias-reducing) estimating function; the default value will result in no regularization.\nnlsolve_arguments...: extra keyword arguments to be passed to NLsolve.nlsolve. See Details.\n\nDetails\n\nBias reduction is either through the solution of the empirically adjusted estimating functions in Kosmidis & Lunardon (2020) (br_method = \"implicit_trace\"; default) or by subtracting an estimate of the bias from the M-estimates (br_method = \"explicit_trace\"). The bias-reducing adjustments and the bias estimate are constructed internally using automatic differentiation (using the ForwardDiff package). \n\nBias reduction for only a subset of parameters can be performed by setting concentrate to the vector of the indices for those parameters.\n\nThe solution of the estimating equations or the adjusted estimating equations is done using the NLsolve package. Keyword arguments can be passed directly to NLsolve.nlsolve through extra keyword arguments. See the NLsolve README for more information on available options.\n\nAn extra additive regularizer to either the estimating functions or the bias-reducing adjusted estimating functions can be suplied via the keyword argument regularizer, which must be a length(theta)-valued function of the parameters and the data; the default value will result in no regularization.\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#StatsAPI.coef","page":"Public","title":"StatsAPI.coef","text":"coef(results::MEstimation_results)\n\nExtract the parameter estimates from a MEstimation_results object.\n\nArguments\n\nresults: a MEstimation_results object.\n\nDetails\n\ncoef(results) returns results.theta\n\n\n\n\n\n","category":"function"},{"location":"lib/public/#StatsAPI.vcov","page":"Public","title":"StatsAPI.vcov","text":"vcov(results::MEstimation_results)\n\nCompute an estimate of the variance-covariance matrix of the M-estimator or its reduced-bias version at results.theta, from a MEstimation_results object.\n\nArguments\n\nresults: a MEstimation_results object.\n\nResult\n\nThe length(coef(results)) times length(coef(results)) estimated variance covariance matrix for the parameters. This matrix is the empirical sandwich variance covariance matrix for M- and RBM-estimators. See, for example, Stefanski and Boos (2002, expression 10).\n\n\n\n\n\n","category":"function"},{"location":"lib/public/#StatsAPI.stderror","page":"Public","title":"StatsAPI.stderror","text":"stderror(results::MEstimation_results)\n\nCompute estimated standard errors from a from a MEstimation_results object.\n\nArguments\n\nresults: a MEstimation_results object.\n\nDetails\n\nThe estimated standard errors are computed as sqrt.(diag(vcov(results))).\n\n\n\n\n\n","category":"function"},{"location":"lib/public/#StatsAPI.coeftable","page":"Public","title":"StatsAPI.coeftable","text":"coeftable(results::MEstimation_results; \n          level::Real=0.95)\n\nReturn a StatsBase.CoefTable from a MEstimation_results object. \n\nArguments\n\nresults: a MEstimation_results object.\n\nKeyword arguments\n\nlevel: a Real that determines the level of the reported confidence intervals; default is 0.95; see Details.\n\nDetails\n\nThe reported confidence intervals are Wald-type of nominal level level, using quantiles of the standard normal distribution.\n\n\n\n\n\n","category":"function"},{"location":"lib/public/#MEstimation.tic","page":"Public","title":"MEstimation.tic","text":"tic(results::MEstimation_results)\n\nCompute the Takeuchi Information Criterion at results.theta, from a MEstimation_results object.\n\nArguments\n\nresults: a MEstimation_results object.\n\nDetails\n\nnothing is returned if results.template is an estimating_function_template.\n\n\n\n\n\n","category":"function"},{"location":"lib/public/#StatsAPI.aic","page":"Public","title":"StatsAPI.aic","text":"aic(results::MEstimation_results)\n\nCompute the Akaike Information Criterion at results.theta, from a MEstimation_results object with an objective_function_template. \n\nArguments\n\nresults: a MEstimation_results object.\n\nDetails\n\nnothing is returned if results.template is an estimating_function_template.\n\n\n\n\n\n","category":"function"},{"location":"lib/public/#MEstimation.slice","page":"Public","title":"MEstimation.slice","text":"slice(results::MEstimation_results,\n      what::Int64;\n      grid::Vector{Float64} = Vector{Float64}(),\n      at::Vector{Float64} = Vector{Float64}(),\n      n_points::Int64 = 50,\n      n_sd::Real = 2)\n\nCompute 1-dimensional slices of objective functions and estimating function surfaces for the parameter what over a grid of points grid, from a MEstimation_results object.\n\nArguments\n\nresults: An MEstimation_results object.\nwhat: the index of the parameter for which to compute a slice for\n\nKeyword arguments\n\ngrid: a Vector{Float64}; if supplied, the slice is computed at each element of grid. The default will result in the automatic calculation of the grid; see Details.\nat: a Vector{Float64} of the sample length as coef(results), specifying the parameter values at which to compute the slice. The default results in computing the slice at coef(results).\nn_points: an Int64 specifying the number of grid points to generate. Applicable only if grid is not supplied; see Details.\nn_sd: an Int64 specifying the number of standard errors to be used for the grid generation. Applicable only if grid is not supplied; see Details.\n\nResult\n\nA Dict with keys \"grid\" and \"slice\", holding grid and the values of the slice at grid, respectively.\n\nDetails\n\nThe default value of grid will result in the automatic calculation of a grid of n_points points, between coef(results)[what] - n_sd * stderror(results)[what] and coef(results)[what] + n_sd * stderror(results)[what].\n\n\n\n\n\n","category":"function"},{"location":"#[MEstimation.jl](https://github.com/ikosmidis/MEstimation.jl)","page":"Home","title":"MEstimation.jl","text":"","category":"section"},{"location":"#Authors","page":"Home","title":"Authors","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Ioannis Kosmidis (author, maintainer)\nNicola Lunardon (author)","category":"page"},{"location":"#Licence","page":"Home","title":"Licence","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MIT License","category":"page"},{"location":"#Package-description","page":"Home","title":"Package description","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MEstimation is a Julia package that implements M-estimation for statistical models (see, e.g. Stefanski and Boos, 2002, for an accessible review) either by solving estimating equations or by maximizing inference objectives, like likelihoods and composite likelihoods (see, Varin et al, 2011, for a review), using user-specified templates of just","category":"page"},{"location":"","page":"Home","title":"Home","text":"the estimating function or the objective functions contributions\na function to compute the number of independent contributions in a given data set","category":"page"},{"location":"","page":"Home","title":"Home","text":"A key feature is the use of those templates along with forward mode automatic differentiation (as implemented in ForwardDiff) to provide methods for reduced-bias M-estimation (RBM-estimation; see, Kosmidis & Lunardon, 2020).","category":"page"},{"location":"","page":"Home","title":"Home","text":"See the documentation for more information, and the examples for a showcase of the functionality MEstimation provides.","category":"page"},{"location":"","page":"Home","title":"Home","text":"See NEWS.md for changes, bug fixes and enhancements.","category":"page"},{"location":"#**MEstimation**-templates","page":"Home","title":"MEstimation templates","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MEstimation has been designed so that the only requirements from the user are to:","category":"page"},{"location":"","page":"Home","title":"Home","text":"implement a Julia composite type for the data;\nimplement a function for computing the number of observations from the data object;\nimplement a function for calculating the contribution to the estimating function or to the objective function from a single observation that has arguments the parameter vector, the data object, and the observation index;\nspecify a MEstimation template (using estimating_function_template for estimating functions and objective_function_template for objective function) that has fields the functions for computing the contributions to the estimating functions or to the objective, and the number of observations.","category":"page"},{"location":"","page":"Home","title":"Home","text":"MEstimation, then, can estimate the unknown parameters by either M-estimation or RBM-estimation.","category":"page"},{"location":"#Examples","page":"Home","title":"Examples","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n    \"man/examples.md\",\n    ]","category":"page"},{"location":"#Documentation","page":"Home","title":"Documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n    \"lib/public.md\",\n    \"lib/internal.md\",\n    ]","category":"page"},{"location":"#main-index","page":"Home","title":"Index","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n    \"lib/public.md\",\n    \"lib/internal.md\",\n    ]","category":"page"},{"location":"#References","page":"Home","title":"References","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Varin C, Reid N, and Firth D (2011). An overview of composite likelihood methods. Statistica Sinica 21(1), 5-42. Link\nKosmidis I, Lunardon N (2020). Empirical bias-reducing adjustments to estimating functions. ArXiv:2001.03786. Link\nStefanski L A and Boos D D (2002). The calculus of M-estimation. The American Statistician(56), 29-38. Link","category":"page"},{"location":"lib/internal/#Internals","page":"Internal","title":"Internals","text":"","category":"section"},{"location":"lib/internal/#Contents","page":"Internal","title":"Contents","text":"","category":"section"},{"location":"lib/internal/","page":"Internal","title":"Internal","text":"Pages = [\"internal.md\"]","category":"page"},{"location":"lib/internal/#Index","page":"Internal","title":"Index","text":"","category":"section"},{"location":"lib/internal/","page":"Internal","title":"Internal","text":"Pages = [\"internal.md\"]","category":"page"},{"location":"lib/internal/#Internals-2","page":"Internal","title":"Internals","text":"","category":"section"},{"location":"lib/internal/","page":"Internal","title":"Internal","text":"MEstimation.MEstimation_results\nMEstimation.show","category":"page"},{"location":"lib/internal/#MEstimation.MEstimation_results","page":"Internal","title":"MEstimation.MEstimation_results","text":"MEstimation_results(results::Union{NLsolve.SolverResults, Optim.MultivariateOptimizationResults, Optim.UnivariateOptimizationResults},\n                    theta::Vector,\n                    data::Any,\n                    template::Union{objective_function_template, estimating_function_template},\n                    regularizer::Function,\n                    br::Bool,\n                    has_objective::Bool,\n                    has_regularizer::Bool,\n                    br_method::String)\n\nComposite type for the output of fit for an objective_function_template or an estimating_function_template.\n\nArguments\n\nresults: a Union{NLsolve.SolverResults, Optim.MultivariateOptimizationResults, Optim.UnivariateOptimizationResults} object holding the optimization results as returned from Optim.optimize or NLsolve.nlsolve.\ntheta: a Vector{Float64} with the M-estimates or their reduced-bias version.\ndata: an object of composite type with all the data required to compute the objective function or the estimating functions; see objective_functionand [estimating_function`](@ref).\ntemplate: an objective_function_template or estimating_function_template object.\nregularizer: a function of theta and data returning either a Vector{Float64} of dimension equal to the number of the estimating functions (for estimating_function_template) or a Float64 (for objective_function_template). See fit for details.\nbr: a Bool; if false then results are from the computation of M-estimates, otherwise from the computation of the RBM-estimates.\nhas_objective: a Bool; if true then template is an objective_function_template. \nhas_regularizer: a Bool; if true then a regularizer function has been used during optimization; see fit.\nbr_method: either \"implicittrace\" (default) or \"explicittrace\"; see fit.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/#Base.show","page":"Internal","title":"Base.show","text":"show(io::IO, \n     results::MEstimation_results; \n     digits::Int64 = 4)\n\nshow method for MEstimation_results objects. \n\nArguments\n\nio: an IO object; see show for details.\nresults: a MEstimation_results object.\n\nKeyword arguments\n\ndigits: an Int64 indicating the number of digits to display for the various summaries. Default is 4.\n\nDetails\n\nIf MEstimation_results.has_objective == true, then the result of aic(results) and tic(results) are also printed.\n\n\n\n\n\n","category":"function"},{"location":"man/examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"man/examples/#Contents","page":"Examples","title":"Contents","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Pages = [\"examples.md\"]\nDepth=3","category":"page"},{"location":"man/examples/#Ratio-of-two-means","page":"Examples","title":"Ratio of two means","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Consider a setting where independent pairs of random variables (X_1 Y_1) ldots (X_n Y_n) are observed, and suppose that interest is in the ratio of the mean of Y_i to  the mean of X_i, that is theta = mu_Y  mu_X, with   mu_X = E(X_i) and mu_Y = E(Y_i) ne 0 (i = 1 ldots n).","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Assuming that sampling is from an infinite population, one way of estimating theta without any further assumptions about the joint distribution of (X_i Y_i) is to set the unbiased estimating equation sum_i = 1^n (Y_i - theta X_i) = 0. The resulting M-estimator is then  hattheta = s_Ys_X where s_X = sum_i = 1^n X_i and s_Y = sum_i = 1^n Y_i. ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"The estimator hattheta is generally biased, as can be shown, for example, by an application of the Jensen inequality assuming that X_iis independent of Y_i, and its bias can be reduced using the empirically adjusted estimating functions approach in Kosmidis & Lunardon (2020). ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"This example illustrates how MEstimation can be used to calculate the M-estimator and its reduced-bias version.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"using MEstimation, Random","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Define a data type for ratio estimation problems","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"struct ratio_data\n    y::Vector\n    x::Vector\nend;","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Write a function to compute the number of observations for objects of type ratio_data.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"function ratio_nobs(data::ratio_data)\n    nx = length(data.x)\n    ny = length(data.y)\n    if (nx != ny) \n        error(\"length of x is not equal to the length of y\")\n    end\n    nx\nend;","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Generate some data to test things out","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Random.seed!(123);\nmy_data = ratio_data(randn(10), rand(10));\nratio_nobs(my_data)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"The estimating function for the ratio theta is ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"sum_i = 1^n (Y_i - theta X_i)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"So, the contribution to the estimating function can be implemented as","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"function ratio_ef(theta::Vector,\n                  data::ratio_data,\n                  i::Int64)\n    data.y[i] .- theta * data.x[i]\nend;","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"The estimating_function_template for the ratio estimation problem can now be set up using ratio_nobs and ratio_ef.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"    ratio_template = estimating_function_template(ratio_nobs, ratio_ef);","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"We are now ready use ratio_template and my_data to compute the M-estimator of theta by solving the estimating equation sum_i = 1^n (Y_i - theta X_i) = 0. The starting value for the nonlinear solver is set to 0.1.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"result_m = fit(ratio_template, my_data, [0.1])","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"fit uses methods from the NLsolve package for solving the estimating equations. Arguments can be passed directly to NLsolve.nlsolve through keyword arguments to the fit method. For example,","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"result_m = fit(ratio_template, my_data, [0.1], show_trace = true)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Bias reduction in general M-estimation can be achieved by solving the adjusted estimating equation sum_i = 1^n (Y_i - theta X_i) + A(theta Y X) = 0, where A(theta) are empirical bias-reducing adjustments depending on the first and second derivatives of the estimating function contributions. MEstimation can use ratio_template and automatic differentiation (see, ForwardDiff) to construct A(theta Y X) and, then, solve the bias-reducing adjusted estimating equations. All this is simply done by","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"result_br = fit(ratio_template, my_data, [0.1], estimation_method = \"RBM\") ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"where RBM stands for reduced-bias M-estimation.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Kosmidis & Lunardon (2020) show that the reduced-bias estimator of theta is tildetheta = (s_Y + s_XYs_X)(s_X + s_XXs_X). The code chunks below tests that this is indeed the result MEstimation returns.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"sx = sum(my_data.x);\nsxx = sum(my_data.x .* my_data.x);\nsy = sum(my_data.y);\nsxy = sum(my_data.x .* my_data.y);\nisapprox(sy/sx, result_m.theta[1])\nisapprox((sy + sxy/sx)/(sx + sxx/sx), result_br.theta[1])","category":"page"},{"location":"man/examples/#Logistic-regression","page":"Examples","title":"Logistic regression","text":"","category":"section"},{"location":"man/examples/#Using-[objective_function_template](@ref)","page":"Examples","title":"Using objective_function_template","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Here, we use MEstimation's objective_function_template to estimate a logistic regression model using maximum likelihood and maximum penalized likelihood, with the empirical bias-reducing penalty in Kosmidis & Lunardon (2020).","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"using MEstimation\nusing Random\nusing Distributions\nusing Optim","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"A data type for logistic regression models (consisting of a response vector y, a model matrix x, and a vector of weights m) is","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"struct logistic_data\n    y::Vector\n    x::Array{Float64}\n    m::Vector\nend","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"A function to compute the number of observations from logistic_data objects is","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"function logistic_nobs(data::logistic_data)\n    nx = size(data.x)[1]\n    ny = length(data.y)\n    nm = length(data.m)\n    if (nx != ny) \n        error(\"number of rows in of x is not equal to the length of y\")\n    elseif (nx != nm)\n        error(\"number of rows in of x is not equal to the length of m\")\n    elseif (ny != nm)\n        error(\"length of y is not equal to the length of m\")\n    end\n    nx\nend","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"The logistic regression log-likelihood contribution at a parameter theta for the ith observations of data data is","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"function logistic_loglik(theta::Vector,\n                         data::logistic_data,\n                         i::Int64)\n    eta = sum(data.x[i, :] .* theta)\n    mu = exp.(eta)./(1 .+ exp.(eta))\n    data.y[i] .* log.(mu) + (data.m[i] - data.y[i]) .* log.(1 .- mu)\nend","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Let's simulate some logistic regression data with 10 covariates","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Random.seed!(123);\nn = 100;\nm = 1;\np = 10\nx = Array{Float64}(undef, n, p);\nx[:, 1] .= 1.0;\nfor j in 2:p\n        x[:, j] .= rand(n);\nend\ntrue_betas = randn(p) * sqrt(p);\ny = rand.(Binomial.(m, cdf.(Logistic(), x * true_betas)));\nmy_data = logistic_data(y, x, fill(m, n));","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"and set up an objective_function_template for logistic regression","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"logistic_template = objective_function_template(logistic_nobs, logistic_loglik)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"The maximum likelihood estimates starting at true_betas are","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"o1_ml = fit(logistic_template, my_data, true_betas, optim_method = NelderMead())","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"fit uses methods from the Optim package internally. Here, we used the Optim.NelderMead method. Alternative optimization methods and options can be supplied directly through the keyword arguments optim_method and optim_options, respectively. For example,","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"o2_ml = fit(logistic_template, my_data, true_betas, optim_method = LBFGS(), optim_options = Optim.Options(g_abstol = 1e-05))","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"The reduced-bias estimates starting at the maximum likelihood ones are","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"o1_br = fit(logistic_template, my_data, coef(o1_ml), estimation_method = \"RBM\")","category":"page"},{"location":"man/examples/#Using-[estimating_function_template](@ref)","page":"Examples","title":"Using estimating_function_template","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"The same results as above can be returned using an estimating_function_template for logistic regression. ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"The contribution to the derivatives of the log-likelihood for logistic regression is","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"function logistic_ef(theta::Vector,\n                     data::logistic_data,\n                     i::Int64)\n    eta = sum(data.x[i, :] .* theta)\n    mu = exp.(eta)./(1 .+ exp.(eta))\n    data.x[i, :] * (data.y[i] - data.m[i] * mu)\nend","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Then, solving the bias-reducing adjusted estimating equations","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"logistic_template_ef = estimating_function_template(logistic_nobs, logistic_ef);\ne1_br = fit(logistic_template_ef, my_data, true_betas, estimation_method = \"RBM\")","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"returns the reduced-bias estimates from maximum penalized likelihood:","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"isapprox(coef(o1_br), coef(e1_br))","category":"page"},{"location":"man/examples/#Bias-reduction-methods","page":"Examples","title":"Bias-reduction methods","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"MEstimation currently implements 2 alternative bias reduction methods, called implicit_trace and explicit_trace. implicit_trace will adjust the estimating functions or penalize the objectives, as we have seen earlier. explicit_trace, on the other hand, will form an estimate of the bias of the M-estimator and subtract that from the M-estimates. The default method is implicit_trace.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"For example, for logistic regression via estimating functions ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"e2_br = fit(logistic_template_ef, my_data, true_betas, estimation_method = \"RBM\", br_method = \"explicit_trace\")","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"which gives slightly different estimates that what are in the implict_trace fit in e1_br. ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"The same can be done using objective functions, but numerical differentiation (using the FiniteDiff package) is used to approximate the gradient of the bias-reducing penalty (i.e. A(theta)).","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"o2_br = fit(logistic_template, my_data, true_betas, estimation_method = \"RBM\", br_method = \"explicit_trace\")\nisapprox(coef(e2_br), coef(o2_br))","category":"page"},{"location":"man/examples/#Regularization","page":"Examples","title":"Regularization","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"MEstimation allows to pass arbitrary regularizers to either the objective or the estimating functions. Below we illustrate that functionality for carrying out ridge logistic regression, and maximum penalized likelihood, with a Jeffreys-prior penalty.","category":"page"},{"location":"man/examples/#Ridge-logistic-regression","page":"Examples","title":"Ridge logistic regression","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"The logistic_template that we defined earlier can be used for doing L2-regularized logistic regression (aka ridge logistic regression); we only need to define a function that implements the L2 regularizer","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"l2_penalty = (theta, data, λ) -> - λ * sum(theta.^2);","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Then, the coefficient path can be computed as","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"lambda = collect(0:0.5:10);\ndeviance = similar(lambda);\ncoefficients = Matrix{Float64}(undef, length(lambda), length(true_betas));\ncoefficients[1, :] = coef(o1_ml);\nfor j in 2:length(lambda)\n    current_fit = fit(logistic_template, my_data, coefficients[j - 1, :],\n                      regularizer = (theta, data) -> l2_penalty(theta, data, lambda[j]))\n    deviance[j] = 2 * current_fit.results.minimum\n    coefficients[j, :] = coef(current_fit)\nend","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"The coefficients versus lambda, and the deviance values are then","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"using Plots\nplot(lambda, coefficients);\nsavefig(\"coef_path1.svg\");","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"(Image: )","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"plot(deviance, coefficients);\nsavefig(\"coef_path2.svg\");","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"(Image: )","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Another way to get the above is to define a new data type that has a filed for lambda and then pass","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"l2_penalty = (theta, data) -> - data.λ * sum(theta.^2)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"to the regularizer argument when calling fit. Such a new data type, though, would require to redefine logistic_loglik, logistic_nobs and logistic_template.","category":"page"},{"location":"man/examples/#Jeffreys-prior-penalty-for-bias-reduction","page":"Examples","title":"Jeffreys-prior penalty for bias reduction","text":"","category":"section"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Firth (1993) showed that an alternative bias-reducing penalty for the logistic regression likelihood is the Jeffreys prior,. which can readily implemented and passed to fit through the regularizer interface that MEstimation provides. The logarithm of the Jeffreys prior for logistic regression is ","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"using LinearAlgebra\n\nfunction log_jeffreys_prior(theta, data)\n    x = data.x\n    probs = cdf.(Logistic(), x * theta)\n    log(det((x .* (data.m .* probs .* (1 .- probs)))' * x)) / 2\nend","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Then, the reduced-bias estimates of Firth (1993) are","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"o_jeffreys = fit(logistic_template, my_data, true_betas, regularizer = log_jeffreys_prior)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Note here, that the regularizer is only used to get estimates. Then all model quantities are computed at those estimates, but based only on logistic_loglik (i.e. without adding the regularizer to it). Kosmidis & Firth (2020) provide a more specific procedure for computing the reduced-bias estimates from the penalization of the logistic regression likelihood by Jeffreys prior, which uses repeated maximum likelihood fits on adjusted binomial data. Kosmidis & Firth (2020) also show that, for logistic regression, the reduced-bias estimates from are always finite and shrink to zero relative to the maximum likelihood estimator.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Regularization is also available when fitting an estimating_function_template. For example, the gradient of the log_jeffreys_prior above is","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"using ForwardDiff\nlog_jeffreys_prior_grad = (theta, data) -> ForwardDiff.gradient(pars -> log_jeffreys_prior(pars, data), theta)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Then the same fit as o_jeffreys can be obtained using estimating functions","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"e_jeffreys = fit(logistic_template_ef, my_data, true_betas, regularizer = log_jeffreys_prior_grad)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"Note here that the value of the estimating functions shown in the output is that of the gradient of the log-likelihood, i.e.","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"logistic_loglik_grad = estimating_function(coef(e_jeffreys), my_data, logistic_template_ef)","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"instead of the regularized estimating functions, which, as expected, are very close to zero at the estimates","category":"page"},{"location":"man/examples/","page":"Examples","title":"Examples","text":"logistic_loglik_grad .+ log_jeffreys_prior_grad(coef(e_jeffreys), my_data)","category":"page"}]
}
